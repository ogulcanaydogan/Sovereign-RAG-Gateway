name: ci

on:
  push:
    branches: ["main"]
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: srg_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      SRG_TEST_POSTGRES_DSN: postgresql://postgres:postgres@localhost:5432/srg_test
      SRG_TEST_REDIS_URL: redis://localhost:6379/0
      SRG_RAG_ALLOWED_CONNECTORS: filesystem,postgres
      SRG_RAG_POSTGRES_DSN: postgresql://postgres:postgres@localhost:5432/srg_test
      SRG_RAG_POSTGRES_TABLE: rag_chunks_ci
      SRG_OPA_SIMULATE_TIMEOUT: "false"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install uv
        run: python -m pip install --upgrade pip uv
      - name: Install dependencies
        run: uv sync --extra dev
      - name: Lint
        run: uv run ruff check .
      - name: Type check
        run: uv run mypy app scripts
      - name: Validate contracts
        run: uv run python scripts/validate_schemas.py
      - name: Version sync check
        run: uv run python scripts/check_version_sync.py
      - name: Prepare RAG fixtures
        run: |
          uv run python scripts/generate_synthetic_healthcare_corpus.py --output-dir benchmarks/data/synthetic_corpus
          uv run python scripts/rag_ingest.py --input-dir benchmarks/data/synthetic_corpus --output artifacts/rag/filesystem_index.jsonl
          uv run python scripts/rag_ingest.py \
            --connector postgres \
            --input-dir benchmarks/data/synthetic_corpus \
            --postgres-dsn "$SRG_TEST_POSTGRES_DSN" \
            --postgres-table "$SRG_RAG_POSTGRES_TABLE"
      - name: Citation eval gate
        run: |
          uv run python scripts/eval_citations.py \
            --dataset benchmarks/data/citation_eval.jsonl \
            --output-json artifacts/benchmarks/citation_eval.json \
            --output-markdown docs/benchmarks/reports/citations-latest.md \
            --threshold 0.95
      - name: PGVector ranking eval gate
        run: |
          uv run python scripts/eval_pgvector_ranking.py \
            --dataset benchmarks/data/pgvector_ranking_eval.jsonl \
            --postgres-dsn "$SRG_TEST_POSTGRES_DSN" \
            --postgres-table "$SRG_RAG_POSTGRES_TABLE" \
            --output-json artifacts/benchmarks/pgvector_ranking_eval.json \
            --output-markdown docs/benchmarks/reports/pgvector-ranking-latest.md \
            --threshold 0.8
      - name: Migration checks
        run: uv run python scripts/check_migration_v020rc1.py
      - name: Webhook dead-letter replay smoke
        run: |
          mkdir -p artifacts/audit
          uv run python - <<'PY'
          import json
          import sqlite3

          conn = sqlite3.connect("artifacts/audit/webhook_dead_letter.db")
          conn.execute(
              """
              CREATE TABLE IF NOT EXISTS webhook_dead_letter (
                  id INTEGER PRIMARY KEY AUTOINCREMENT,
                  timestamp TEXT NOT NULL,
                  event_type TEXT NOT NULL,
                  endpoint_url TEXT NOT NULL,
                  status_code INTEGER,
                  error TEXT,
                  attempt_count INTEGER NOT NULL,
                  idempotency_key TEXT NOT NULL,
                  body_json TEXT NOT NULL
              )
              """
          )
          conn.execute(
              """
              INSERT INTO webhook_dead_letter (
                  timestamp, event_type, endpoint_url, status_code, error,
                  attempt_count, idempotency_key, body_json
              ) VALUES (
                  datetime('now'), 'policy_denied', 'https://example.invalid/webhook',
                  500, 'simulated', 1, 'orig-key',
                  ?
              )
              """,
              (json.dumps({"event_type": "policy_denied", "payload": {"request_id": "req-ci"}}),),
          )
          conn.commit()
          conn.close()
          PY
          uv run python scripts/replay_webhook_dead_letter.py \
            --dead-letter artifacts/audit/webhook_dead_letter.db \
            --dead-letter-backend sqlite \
            --dry-run \
            --report-out artifacts/audit/webhook_replay_report.json
      - name: Governance benchmark gate
        run: |
          uv run python scripts/benchmark_runner.py \
            --scenario enforce_redact \
            --dataset benchmarks/data/synthetic_prompts.jsonl \
            --out artifacts/benchmarks/governance
          uv run python scripts/check_benchmark_thresholds.py \
            --summary artifacts/benchmarks/governance/results_summary.json \
            --max-leakage-rate 0.005 \
            --max-latency-p95-ms 250 \
            --max-cost-drift-pct 5 \
            --min-citation-presence-rate 0.5
          uv run python scripts/benchmark_runner.py \
            --scenario provider_429_storm \
            --dataset benchmarks/data/synthetic_prompts.jsonl \
            --out artifacts/benchmarks/faults/provider_429_storm
          uv run python scripts/check_benchmark_thresholds.py \
            --summary artifacts/benchmarks/faults/provider_429_storm/results_summary.json \
            --max-leakage-rate 0.01 \
            --max-latency-p95-ms 400 \
            --max-cost-drift-pct 15 \
            --min-citation-presence-rate 0.0 \
            --min-fault-attribution-accuracy 0.9 \
            --max-detection-delay-ms-p95 600 \
            --max-slo-burn-prediction-error-pct 10 \
            --max-false-positive-incident-rate 0.08
      - name: Governance benchmark trend gate
        run: |
          uv run python scripts/check_benchmark_trend.py \
            --current artifacts/benchmarks/governance/results_summary.json \
            --baseline benchmarks/baselines/governance_enforce_redact_summary.json \
            --max-latency-regression-pct 20 \
            --max-leakage-regression-abs 0.002 \
            --max-abs-cost-drift-regression-pct 3 \
            --max-citation-drop-abs 0.1
      - name: Tests
        run: uv run pytest

  provider-parity-matrix:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        provider: [http_openai, azure_openai, anthropic]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install uv
        run: python -m pip install --upgrade pip uv
      - name: Install dependencies
        run: uv sync --extra dev
      - name: Generate provider parity matrix entry
        run: |
          uv run python scripts/provider_parity_matrix.py \
            --provider "${{ matrix.provider }}" \
            --strict \
            --out-json "artifacts/provider-parity/${{ matrix.provider }}.json" \
            --out-markdown "artifacts/provider-parity/${{ matrix.provider }}.md"
      - name: Upload provider parity artifact
        uses: actions/upload-artifact@v4
        with:
          name: provider-parity-${{ matrix.provider }}
          path: artifacts/provider-parity/

  provider-parity-summary:
    runs-on: ubuntu-latest
    needs: provider-parity-matrix
    steps:
      - uses: actions/download-artifact@v4
        with:
          pattern: provider-parity-*
          path: artifacts/provider-parity
          merge-multiple: true
      - name: Build parity summary matrix view
        run: |
          python - <<'PY'
          import json
          from pathlib import Path

          root = Path("artifacts/provider-parity")
          rows = []
          for path in sorted(root.glob("*.json")):
            payload = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(payload, list):
              rows.extend(payload)

          lines = [
            "# Provider Parity Matrix (CI Summary)",
            "",
            "| Provider | Status | Chat | Embeddings | Streaming | Chat shape | Embeddings shape | Stream shape |",
            "|---|---|---:|---:|---:|---:|---:|---:|",
          ]
          for item in rows:
            lines.append(
              f"| {item['provider']} | {item['status']} | "
              f"{'yes' if item['chat_supported'] else 'no'} | "
              f"{'yes' if item['embeddings_supported'] else 'no'} | "
              f"{'yes' if item['streaming_supported'] else 'no'} | "
              f"{'yes' if item['chat_shape_ok'] else 'no'} | "
              f"{'yes' if item['embeddings_shape_ok'] else 'no'} | "
              f"{'yes' if item['stream_shape_ok'] else 'no'} |"
            )
          lines.append("")

          summary = "\n".join(lines).strip() + "\n"
          out_path = root / "matrix-summary.md"
          out_path.write_text(summary, encoding="utf-8")
          print(summary)
          PY
      - name: Publish job summary
        run: cat artifacts/provider-parity/matrix-summary.md >> "$GITHUB_STEP_SUMMARY"
      - name: Upload summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: provider-parity-summary
          path: artifacts/provider-parity/matrix-summary.md
